# Advanced Retrieval Evaluation System

This system provides a modular approach to evaluate different advanced retrieval methods using RAGAS metrics and LangSmith tracing.

## ğŸš€ Quick Start

### 1. **First time**: Run `prepare_shared_state.py` once
```bash
cd financial_advisor_assistant
source .venv/bin/activate
python prepare_shared_state.py
```

### 2. **Test LangSmith Tracing** (Optional but Recommended)
```bash
python test_langsmith.py
```
This will test if LangSmith tracing is working properly. You'll need a LangSmith API key.

### 3. **Run Individual Evaluations**
```bash
# Multi-Query Retrieval
python evaluate_multi_query.py

# Ensemble Retrieval  
python evaluate_ensemble.py

# Parent Document Retrieval
python evaluate_parent_document.py

# BM25 Retrieval
python evaluate_bm25.py
```

## ğŸ“ File Structure

```
financial_advisor_assistant/
â”œâ”€â”€ prepare_shared_state.py          # Prepares shared resources
â”œâ”€â”€ evaluate_multi_query.py          # Multi-Query retriever evaluation
â”œâ”€â”€ evaluate_ensemble.py             # Ensemble retriever evaluation  
â”œâ”€â”€ evaluate_parent_document.py      # Parent Document retriever evaluation
â”œâ”€â”€ evaluate_bm25.py                 # BM25 retriever evaluation
â”œâ”€â”€ test_langsmith.py               # LangSmith tracing test
â”œâ”€â”€ shared_state/                   # Shared resources (created by prepare_shared_state.py)
â”‚   â”œâ”€â”€ documents.pkl
â”‚   â”œâ”€â”€ split_documents.pkl
â”‚   â”œâ”€â”€ golden_dataset.csv
â”‚   â””â”€â”€ vector_store_info.pkl
â””â”€â”€ results/                        # Evaluation results (created by evaluation scripts)
    â”œâ”€â”€ multi_query_results_TIMESTAMP.json
    â”œâ”€â”€ ensemble_results_TIMESTAMP.json
    â”œâ”€â”€ parent_document_results_TIMESTAMP.json
    â””â”€â”€ bm25_results_TIMESTAMP.json
```

## ğŸ”§ LangSmith Setup

### Getting a LangSmith API Key
1. Go to [https://smith.langchain.com](https://smith.langchain.com)
2. Sign up or log in
3. Go to Settings â†’ API Keys
4. Create a new API key

### Testing LangSmith
Before running evaluations, test LangSmith tracing:
```bash
python test_langsmith.py
```

### LangSmith Projects
Each evaluation script uses a different LangSmith project:
- `financial_advisor_shared_state_preparation`
- `financial_advisor_multi_query_evaluation`
- `financial_advisor_ensemble_evaluation`
- `financial_advisor_parent_document_evaluation`
- `financial_advisor_bm25_evaluation`

## ğŸ“Š Evaluation Metrics

Each evaluation uses these RAGAS metrics:
- **Faithfulness**: Measures if the response is faithful to the retrieved context
- **Response Relevancy**: Measures if the response is relevant to the question
- **Context Precision**: Measures if the retrieved context is precise
- **Context Recall**: Measures if the retrieved context covers the answer

## ğŸ’¡ Benefits

### Efficiency
- **Shared Resources**: Documents, golden dataset, and vector store are created once
- **Isolated Failures**: If one retriever fails, others continue
- **No Redundant Work**: Expensive steps (document loading, dataset generation) run once

### LangSmith Integration
- **Separate Projects**: Each retriever has its own LangSmith project for clean separation
- **Detailed Traces**: Full tracing of LLM calls, retrievals, and evaluations
- **Cost Analysis**: Track costs and latency for each retriever
- **Debugging**: Easy to debug issues with detailed traces

### Modularity
- **Independent Scripts**: Each retriever evaluation is self-contained
- **Easy Extension**: Add new retrievers by copying and modifying existing scripts
- **Clear Separation**: Each script focuses on one retriever type

## ğŸ” Troubleshooting

### LangSmith Issues
If LangSmith tracing isn't working:
1. Check your API key is correct
2. Run `python test_langsmith.py` to test
3. Check network connectivity
4. Verify LangSmith service is up

### Missing Dependencies
If you get import errors:
```bash
pip install langsmith langchain-openai ragas qdrant-client
```

### Shared State Issues
If shared state is missing:
```bash
python prepare_shared_state.py
```

## ğŸ“ˆ Expected Output

Each evaluation will produce results like:
```json
{
  "faithfulness": 0.9566,
  "response_relevancy": 0.8956, 
  "context_precision": 0.9583,
  "context_recall": 0.6247
}
```

Plus detailed traces in LangSmith for cost and latency analysis. 